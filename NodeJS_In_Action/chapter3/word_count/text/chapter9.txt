Virtual Memory

Processes in a system share the CPU and main memory with other processes
  - however, sharing the main memory poses some special challeneges
  - as demand on the CPU increases, processes slow down in some reasonable smooth way
  - if too many processes need too much memory, then some of them will not be able to run
  - when a program is out of space, it is out of luck
  - memory is also vulnerable to corruption
  - if some process inadvertently writes to the memory used by another process, that process might fail in some
    bewildering fashion totally unrelated to the program logic

In order to manage memory more efficiently and with fewer errors, modern systems provide an abstraction of main
memory known as virtual memory (VM)
  - virtual memory is an elegant interation of hardware exceptions, hardware address translation, main memory, disk
    files, and kernel software that provides each process with a large, uniform, and private address space
  - with one clean mechanism, virtual memory provides three important capabilities:
      1.  It uses main memory efficiently by treating it as a cache for an address space stored on disk, keeping only
	  the active areas in main memory, and transferring data back and forth between disk and memory as needed
      2.  it simplifies memory management by providing each process with a uniform address space
      3.  It protects the address space of each process from corruption by other processes

Virtual memory is one of the great ideas in computer science
  - a major reason for its success is that it works silently and automatically, without any intervention from the
    application programmer
  - since virtual memory woks so well behind the scenes, why should a programmer need to understand it?
  - there are several reasons:
      *	virtual memory is central: virtual memory pervades all levels of computer systems playing key roles in the
	design of hardware exceptions, assemblers, linkers, loaders, shared objects, files and processes.
	Understanding virtual memory will help you better understnad how system work in general
      *	virtual memory is powerful: virtual memory gives applications powerful capabilities to create and destroy
	chunks of memory, map chunks of memory to portions of disk files, and share memory with other processes. For
	example, did you know that you can read or modify the contents of a disk file by reading and writing memory
	locations? Or that you can load the contents of a file into memory without doing any explicit copying?
	Understanding virtual memory will help you harness its powerful capabilities in your applications.
      *	virtual memory is dangerous: applications interact with virtual memory every time the reverence a variable,
	dereference a pointer, or make a call to a dynamic allocation package such as malloc. If virtual memory is
	used improperly, applications can suffer from perplexing and insidious memory-related bugs. For example, a
	program with a bad pointer can crash immediately with a "Segmentation fault" or a "Protection fault", run
	silently for hours before crashing, or scariest of all, run to completion with incorrect results.
	Understanding virtual memory, and the allocation packages such as malloc that manage it, can help you avoid
	these errors.

Physical and Virtual Addressing

The main memory of a computer system is organized as an array of M contiguous byte-sized cells
  - each byte has a unique physical address (PA)
  - the first byte has an address of 0, the next byte has an address of 1, the next byte has an address of 2, and
    so on
  - given this simple organization, the most natural way for a CPU to access memory would be to use physical
    addresses
  - we call this approach physical addressing
  - when the CPU executes the load instruction, it generates an effective physical address and passes it to main
    memory over the memory bus
  - the main memory fetches the 4-byte word starting at a predefined physical address and returns it to the CPU,
    which stores it in a register

Early PCs used physical addressing, and systems such as digital signal processors, embedded microcontrollers, and
Cray supercomputers continue to do so
  - modern processors use a form of addressing known as virtual addressing

With virtual addressing:
  - the CPU accesses main memory by generating a virtual address (VA), which is converted to the appropriate physical
    address before being sent to the memory
  - the task of converting a virtual address to a physical one is known as address translation
  - like exception handling, address translation requires close cooperation between the CPU hardware and the
    operating system
  - dedicated hardware on the CPU chip called the memory management unit (MMU) translates virtual addresses on the
    fly, using a look-up tables stored in main memory whose contents are managed by the operating system

Address Spaces

An address space is an ordered set of nonnegative inter addresses:

      {0, 1, 2, ...}

  - if the integers in the address space are consecutive, then it is a linear address space
  - in order to simplify our discussion we will always asssume linear address spaces
  - in a system with virtual memory, the CPU generates virtual addresses from an address space of N = 2^n addresses
    called the virtual address space:

      {0, 1, 2, ..., N - 1}

  - the size of an address space is characterized by the number of bits that are needed to represent the
    largest address
      * i.e., a virtual address space with N = 2^n addresses is called an n-bit address space
      *	modern systems typically support either 32-bit or 64-bit virtual address spaces

A system also has a physical address space that corresponds to the M bytes of physical memory in the system:

      {0, 1, 2, ..., M - 1}

  - M is not required to be a power of two, but to simplify the discussion we will assume that M = 2^m

The concept of an address space is important b/c it makes a clean distinction between data objects (bytes) and their
attributes (addresses).
  - once this distinction is recognized, then we can generalize and allow each data object to have multiple
    independent addresses, each chosen from a different address space

This is the basic idea of virtual memory
  - each byte of main memory has a virtual address chosen from the virtual address space, and a physical address
    chosen from the physical address space

VM as a Tool for Caching

Conceptually a virtual memory is organized as an array of N contiguous byte-sized cells stored on disk
  - each byte has a unique address that serves as an index into the array
  - the contents of the array on disk are cached into main memory
  - like any other cache in the memory hierarchy, the datat on disk (the lower level) is partitioned into blocks that
    serve as the transfer units between the disk and main memory (the upper level)
  - VM systems handle this by partitioning the virtual memory into fixed sized blocks called virtual pages (VPs),
    each virtual page is P = 2^p bytes in size
  - similarly, physical memory is partitioned into physical pages (PPs), also P bytes in size (physical pages are
    also referred to as page frames)

At any point in time, the set of virtual pages is partitioned into three disjoint subsets
  - Unallocated:  Pages that have not yet been allocated (or created) by the VM system. Unallocated blocks do not
    have any data associated with them, and this do not occupy any space on disk
  - Cached:   Allocated pages that are currently cahced in physical memory
  - Uncached:	Allocated pages that are not cached in physical memory

As an example in figure 9.3 you have a small virtual memory with eight virtual pages
  - pages 0 and 3 have not been allocated yet and do not exist on disk
  - vitual pages 1, 4, and 6 are cached in physical memory
  - pages 2, 5, and 7 are allocated, but are not currently cached in main memory

DRAM Cache Organization

TO help us keep the different caches in the memory hierarchy straight, we will use the term SRAM cache to denote the
L1, L2, and L3 cache memories between the CPU and main memory, and the term DRAM cache to denote the VM system's
cache that caches virtual pages in main memory

The position of the DRAM cache in the memory hierarchy has a big impact on the way that it is organized
  - DRAM is at least 10 times slower than SRAM and that disk is about 100,000 times slower than a DRAM
  - misses in DRAM caches are very expensive compared to misses in SRAM caches b/c DRAM cache misses are served from
    disk, SRAM cache misses are usually served from DRAM-based memory
  - furthermore, the cost of reading the first byte from a disk sector is about 100,000 times slower than reading
    successive bytes in the sector
  - the bottom line is that the organization of the DRAM cache is driven entirely by the enormous cost of misses

B/c of the large miss penalty and the expense of accessing the first byte, virtual pages tend to be large, typically
4 KB to 2 MB
  - due to the large miss penalty, DRAM caches are fully associative, that is any virtual page can be placed in any
    physical page
  - the replacement policy on misses also assumes greater importance, b/c the penalty associated with replacing the
    wrong virtual pages is so high
  - operating system use much more sophisticated replacement algorithms for DRAM caches than the hardware does for
    SRAM caches
  - the replacement algorithms are beyond the scope of the course
  - b/c of the large access time of disk, DRAM caches always use write-back instead of write-through

Page tables

As with any cache, the VM system must have some way to determine if a virtual page is cached somewhere in DRAM
  - if so, the system must determine which physical page it is cached in
  - if there is a miss, the system must determine where the virtual page is stored on disk, select a victim page in
    physical mempory, and copy the virtual page from disk to DRAM, replacing the victim page

These capabilities are provided by a combination of operating system software, address translation hardware in the
MMU (memory mangement unit), and a data structure stored in pohysical memory known as the page table that maps
virtual pages to physical pages
  - the address translation hardware reads the page table each time it converts a virtual address to a physical
    address
  - the operating system is responsible for maintaining the contents of the page table and transferring pages back
    and forth between disk and DRAM

Figure 9.4 shows the basic organization of a page table
  - a page table is an array of page table entries (PTEs)
  - each page in the virtual address space has a PTE at a fixed offset in the page table
  - for our puposes, we will assume that each PTE consists of a valid bit and an n-bit address field
  - the valid bit indicates whether the virtual page is currently cached in DRAM
  - if the valid bit is set, the address field indicates the start of the corresponding physical page in DRAM where
    the virtual page is cached
  - if the valid bit is not set, then a null address indicates that the virtual page has not yet been allocated
  - otherwise, the addrss points to the start of the virtual page on disk
  - an important point to notice about figure 9.4 is that because the DRAM cache is fully associative, and physical
    page can contain any virtual page

Page Hits

Consider what happens when the CPU reads a word from virtual memory contained in VP 2, which is cached in DRAM,
see Figure 9.5
  - using a technique we will describe in detail in section 9.6, the address translation hardware uses the virtual
    address as an index to locate PTE 2 and read it from memory
  - since the valid bit is set, the address translation hardware knows that VP 2 is cached in memory
  - it therefore uses the physical memory address in the PTE (which points to the start of the cached page in PP 1)
    to construct the physical address of the word

Page Faults

In virtual memory parlance, a DRAM cache miss is known as a page fault
  - figure 9.6 shows the state of our example page table before the fault, the CPU has referenced a word in VP 3,
    which is not cached in DRAM
  - the address translation hardware reads PTE 3 from memory, infers from the valid bit that VP 3 is not cached, and
    triggers a page fault exception

The page fault exception invokes a page fault exception ahndler in the kernel, which selects a victim page, in this
case VP 4 stored in PP 3
  - if VP 4 has been modified, then the kernel copies it back to disk
  - in either case, the kernel modifies the page table entry for VP 4 to reflect the fact that VP 4 is no longer in
    main memory
  - next the kernel copies VP 3 from disk to PP 3 in memory, updates PTE 3, and then returns
  - when the handler returns, it restarts the faulting instruction, which resends the faulting virtual address to the
    address translation hardware
  - but now, VP 3 is cached in main memory, and the page hit is handled normally by the address translation hardware
  - figure 9.7 shows the state of our example page table after the page fault

Virtual memory was invented in the early 1960s, long before the widening CPU-memory gap spawned SRAM caches
  - as a result, virtual memory systems use a different terminology from SRAM caches, even though many of the ideas
    are similar
  - in virtual memory parlance, blocks are known as pages
  - the activity of transfering a page between disk and memory is known as swapping or paging
  - pages are swapped in (paged in) from disk to DRAM, and swapped out (paged out) from DRAM to disk
  - the strategy of waiting until the last moment to swap in a page, when a miss occurs, is known as demand paging
  - other approaches, such as trying to predict misses and swap pages in before they are actually referenced, are
    possible
  - all modern systems use demand paging

Allocating Pages

Figure 9.8 shows the effect on our example page table when the operating system allocates a new page of virtual mem-
ory, i.e. as a result of calling malloc
  - in the example VP 5 is allocated by creating room on disk and updating PTE 5 to point to the newly created page
    page on disk

Locality to the Rescue Again

When many of us learn the idea of virtual memory, our first impression is often that it must be terribly inefficient
  - given the large miss penalties, we worry that apging will destroy program performance
  - in practice, virtual memory works well, b/c of our old friend locality

Although the total number of distinct pages that programs reference during an entire run might exceed the total size
of physical memory, the principle of locality promises that at any point in time they will ten to work on a smaller
set of active pages known as the working set or resident set
  - after an initial overhead where the working set is paged into memory, subsequent references to theworking set
    result in hits, with no additional disk traffic

As long as our program have good temporal locality, virtual memory systems work quite well
  - of course, not all programs exhibit good temporal locality
  - if the working set size exceeds the size of physical memory, then the program can produce an unfortunate
    situation known as thrashing, where pages are swapped in and out continuously
  - although virtual memory is usually efficient, if a program's performance slows to a crawl, the wise programmer
    will consider the possibility that it is thrashing

Counting Pafe Faults

You can monitor the number of page faults (and lots of other information) with the Unix getrusage function

VM as a Tool for Memory Management

In the last section, we saw how virtual memory provides a mechanism for using the DRAM to cache pages from a
typically larger virtual address space
  - interestingly, some early systems such as the DEC PDP-11/70 supported a virtual address space that was smaller
    than the available physical memory
  - yet virtual memory was still a useful mechanism b/c it greatly simplified memory management and provided a
    natural way to protect memory

Thus far, we have assumed a single page table that maps a single virtual address space to the physical address space
  - in fact, operating systems provide a separate page table, and thus a separate virtual address space for each
    process
  - figure 9.9 shows the basic idea
  - in the example, the page table for process i maps VP 1 to PP 2 and VP 2 to PP 7
  - similarly, the page table for process j maps VP 1 to PP 7 amd VP 2 to PP 10
  -  notice that multiple virtual pages can be mapped to the same shared physical page

The combination of demand paging and separate virtual address spaces has a profound impact on the way that memory
is used and managed in a system
  - in particular, VM simplifies linking and loading, the sharing of code and data, and allocating memory to applica-
    tions

Simplifying linking: A separate address space allows each process to use the same basic format for its memory image,
regardless of where the code and data actually reside in physical memory. For example, every process on a given Linux
system has a similar memory format
  - the text section always starts at virtual address 0x08048000 (for a 32-bit address space), or at address
    0x400000 fir a 64-bit address space)
  - the data and bss sections follow immediately after the text section
  - the stack occupues the highest portion of the process space and grows downward
  - such uniformity greatly simplifies the design and implementation of linkers, allowing them to produce fully
    linked executables that are independent of the ultimate location of the code and data in physical memory

Simplified loading: Virtual memory also makes it easy to load executable and shared object filed into memory
  - the .text and .data sections in ELF executable are contiguous
  - to load these sections into a newly created process, the Linux loader allocated a contiguous chunk of virtual
    pages starting at address 0x08048000 (32-bit address spaces) or 0x40000 (64-bit address spaces), marks them as
    invalid (i.e., not cached), and points their page table entries to the appropriate location in the object file
  - the interesting point is that the loader never actually copies and data from disk into memory
  - the data is paged in automatically and on demand by the virtual memory system the first time each page is refer-
    enced, either by the CPU when it fetches an instruction, or by an execuitng instruction when it references a
    memory location

This notion of mapping a set of contiguous virtual pages to an arbitrary location in an arbitrary file is known as
memory mapping
  - Unix provides a system call called mmap that allows application program to do their own memory mapping
  - we will describe application-level memory mapping in more detail in Section 9.8

Simplifying sharing: Separate address spaces provide the operating system with a consistent mechanism for managing
sharing between user processes and the operating system itself
  - in general, each process has its own provate code, data, heap, and stack areas that are not shared with any other
    process
  - in this case, the operating system creates page tables that map the corresponding virtual pages to disjoint
    physical pages
  - in some instances it is desirable for processes to share code and data
  - every process must call the same operating system kernel code, and every C program makes calles to routines in
    the standard C library such as printf
  - rather than including separate copies of the kernel and standard C library in each process, the operating system
    can arrange for multiple processes to share a single copy of this code by mapping the appropriate virtual pages
    in different processes to the same physical pages

Simplifying memory allocation: Virtual memory provides a simple mechanism for allocating additional memory to user
processes
  - when a program running in a user process requests additional heap space (e.g. as a result of calling malloc), the
    operating system allocated an appropriate number, say k, of contiguous virtual memory pages, and maps them to k
    arbitrary physical pages located anywhere in physical memory
  - because of the way page tables work, there is no need for the operating system to locate k contiguous pages of
    physical memory
  - the pages can be scattered randomly in physical memory

VM as a Tool for Memory Protection

Any modern computer system must provide the means for the operating system to control access to the memory systme
  - a user process should not be allowed to modify its read-only text section
  - nor should it be allowed to read or modify any of the code and data structures in the kernel
  - it shoudl not be allowed to read or write the private memory of other processes, and it should not be allowed
    to modify any virtual pages that are shared with other processes, unless all parties explicitly allow it (via
    calls to explicit interprocess communication system calls)

As we have seen, providing separate virtual address spaces makes it easy to isolate the provate memories of different
processes
  - but the address translation mechanism can be extended in a natural way to provde even finer access control
  - since the address translation hardware reads a PTE each time the CPU generates an address, it is straightforward
    to control access to the contents of a virtual page by adding some additional permission bits to the PTE
  - figure 9.10 shows the general idea

In this example, we have added three premission bits to each PTE
  - the SUP bit indicates whether processes must be running in kernel (supervisor) mode to access the page
  - processes running in kernel mode can access any page, bit processes running in user mode are only allowed to
    access pages for which SUP is 0
  - the READ and WRITE bits control read and write access to the page
  - for example, if process i is running in user mode, then it has permission to read VP 0 and to read or write VP 1
  - however it is not allowed to access VP 2

If an instruction violates these permission, then the CPU triggers a general protection fault that transers control
to an exception handler in the kernel
  - Unix shells typically report thus exception as a "segmentation fault"

Address Translation

This section covers the basics of address translation
  - the aim is to give you an appreciation of the hardware's role in supporting virtual memory, with enough detail so
    that you are able to work through some concrete examples by hand
  - keep in mind that we are omitting a number of details, especially related to timing, which are important to
    harware designers but are beyond our scope
  - Figure 9.11 summarizes the symbols that we will be using throughout the section

Formally, address translation is a mapping between the elements of an N-element virtual address space (VAS) and an
M-element physical address space (PAS)
  - Figure 9.12 shows how the MMU uses the page table to perform this mapping
  - a control register in the CPU, the page table base register (PTBR) points to the current page table
  - the n-bit virtual address has two components:
      *	a p-bit virtual page offset (VPO)
      *	an (n - p)-bit virtual page number (VPN)
  - the MMU uses the VPN to select the appropriate PTE
      *	VPN 0 selects PTE 0, VPN 1 selects PTE 1, and so on
  - the corresponding physical address is the concatenation of the physical page number (PPN) from the page table
    entry and the VPO from the virtual address
  - notice that since the physical and virtual pages are both P bytes, the physical page offset (PPO) is identical to
    the VPO

Figure 9.13(a) shows the steps that the CPU hardware performs when there is a page hit
  - Step 1: the processor generate a virtual address and sends it to the MMU
  - Step 2: the MMU generates the PTE address (PTEA) and requests it from the cache/main memory
  - Step 3: the cache/main memory returns the PTE to the MMU
  - Step 3.5: the MMU constructs the physical address (PA) and sends it to the cache/main memory
  - Step 4: the cache/main memory returns the requested data word to the processor

Unlike a page hit, which is handled entirely by hardware, handling a page fault requires cooperation between hardware
and the operating system kernel, see Figure 9.14(b)
  - Steps 1 to 3: The same as Steps 1 to 3 in Figure 9.13(a)
  - Step 4: the valid bit in the PTE is zero, so the MMU triggers an exception, which transfers control in the CPU
    to a page fault exception handler in the operating system kernel
  - Step 5: The fault handler identifies a victim page in physical memory, and if that page has been modified, pages
    it out to disk
  - Step 6: The fault handler pages in the new page and updates the PTE in memory
  - Step 7: The fault handler returns to the original process, causing the faulting instruction to be restarted.
    The CPU resends the offending virtual address to the MMU. B/c the virtual page is now cached in physical
    memory, there is a hit, and after the MMU performs the steps in Figure 9.13(b), the main memory returns the
    requested word to the processor

Integrating Caches and VM

In any system that uses both virtual memory and SRAM caches, there is the issue of whether to use virtual or physical
addresses to access the SRAM cache
  - although a detailed discussion of the trade-offs is beyond our scope here, most systems opt for physical
    addressing
  - with physical addressing, it is straighforward for multiple processes to have blocks in the cache at the same
    time and to share blocks from the same virtual pages
  - further, the cache does not have to deal with protection issues because access rights are checked as part of the
    address transaltion process

Figure 9.14 shows how a physically addressed cache might be integrated with virtual memory
  - the main idea is that the address translaiton occurs before the cache lookup
  - notice that page table entires can be cached, just like any other data words

Speeding up Address Translation with a TLB

As we have seen, everytime the CPU generates a virutal address, the MMU must refer to a PTE in order to translate
the virtual address into a physical address
  - in the worst case, this requires an additional fetch from memory, at a cost of tens of hundreds of cycles
  - if the PTE happens to be cached in L1, then the cost goes down to one or two cycles
  - however, many systems try to eliminate even this cost by including a small cache of PTEs in the MMU called a
    translation lookaside buffer (TLB)

A TLB is a small, cirtually addressed cache where each line holds a block consisting of a single PTE
  - a TLB usually has a high degree of associativity
  - as shown in Figure 9.15 the index and tag fields that are used for set selection and line matching are extracted
    from the virtual page number in the virtual address
  - If the TLB has T = 2^t sets, then the TLB index (TLBI) consists of the t least significant bits of the VPN, and
    the TLB tag (TLBT) consists of the remaining bits in the VPN

Figure 9.16(a) shows the steps involved when there is a TLB hit (the usual case)
  - the key point here is that all of the address translation steps are performed inside the on-chip MMU, and thus
    are fast
      *	Step 1:	The CPU generates a virtual address
      *	Step 2 and 3: The MMU fetches the appropriate PTE from the TLB
      *	Step 4:	The MMU translates the virtual address to a physical address and sends it to the cache/main memory
      *	Step 5:	The cache/main memory returns the requested data word to the CPU

When there is a TLB miss, then the MMU must fetch the PTE from the L1 cache, as shown in Figure 9.15(b).
  - The newly fetched PTE is stored in the TLB, possible overwritting an existing entry

Multi-Level Page Tables

To this point we have assumed that the system uses a single page table to do address translation
  - but if we had a 32-bit address space, 4 KB pages, and a 4-byte PTE, then we would need a 4 MB table resident
    in memory at all time, even if the application referenced only a small chunk of the virtual address space
  - the problem is compounded for systems with 64-bit adress spaces

The common approach for compacting the page table is to use a hierarchy of page tables instead
  - the idea is easiest to understnad with a concrete example
  - consider a 32-bit virtual address space paritioned into 4 KB pages, with page table entires that are 4 bytes each
  - suppose also that at this point in time the virtual addrss space has the following form:
      *	the first 2K pages of memory are allocated for code and data
      *	the next 6K pages are unallocated
      * the next 1023 pages are alos unallocated
      * and the next apge is allocated for the user stack
  - Figure 9.17 shows how we might construct a two-level page table hierarchy for this virtual address space

Each PTE in the level-1 table is responsible for mapping a 4MB chunk of virtual address space, where each chunk
consists of 1024 contiguous pages
  - for example, PTE 0 maps the first chunk, PTE 1 the next chunk, and so on
  - given that the address space is 4 GD, 1024 PTEs are sufficient to cover the entire space

If every page in chunk i is unallocated, the level 1 PTE i is null
  - for example in Figure 9.17, chunks 2-7 are unallocated
  - however, if at least one page in chunk i is allocated, the level 1 PTE i points to the base of a level 2
    page table
  - for example in Figure 9.17 all or portions of chunks 0, 1, and 8 are allocated so their level 1 PTEs point to
    level 2 page tables

Each PTE in a level 2 page table is responsible for mapping a 4 KB page of virtual memory, just as beofre when we
looked at single-level page tables
  - notice that with 4-byte PTEs, each level 1 and level 2 page table is 4K bytes, which conveniently is the same
    size as a page

This scheme reduces memory requirements in two ways
  1.  if a PTE in the level 1 table is null, then the corresponding level 2 page table does not have to exist. This
      represents a significant potential savings, since most of the 4 GB virtual address space for typical program
      is unallocated.
  2.  Only the level 1 table needs to be in main memory at all times. The level 2 page tables can be created and
      and paged in and out by the VM system as they are needed, which reduces pressure on main memory. Only the most
      heavily used level 2 page tables need to be cached in main memory.

Figure 9.18 summarizes address translation with a k-levle page table hierarchy
  - the virtual address is partitioned into k VPNs and a VPO
  - Each VPNi, 1 <= i <= k, is an index into a page table at level i
  - each PTE in a level-j table, 1 <= j <= k - 1, points to the base of some page table at level j + 1
  - each PTE in a level-k table contains either the PPN of some physical page or the address of a disk block
  - to construct the physical address, the MMU must access k PTEs before it can determine the PPN
  - as with a single-level hierarchy, the PPO is identical to the VPO

Accessing k PTEs may seem expensive and impractical at first glance
  - however, the TLB comes to the rescue here by caching PTS from the page tables at the different levels
  - in practice, address transaltion with multi-level page table is not significantly slower than with single-level
    page tables

Putting it Together: End-to-end Address Translation

In this section, we put it all together with a concrete example of end-to-end address translation on a small system
with a TLB and L1 d-cache
  - to keep thins manageable, we will make the following assumptions
      *	the memory is byte addressable
      *	memory accesses are to 1-byte words (not 4-byte words)
      * virtual addresses are 14 bits wide (n = 14)
      * physical addresses are 12 bits wide (m = 12)
      *	the page size if 64 bytes (P = 64)
      *	The TLB is four-way set associative with 16 total entries
      *	the L1 d-cache is physically addressed and direct mapped, with a 4-byte line size and 16 total sets

Figure 9.19 shows the formate of the virtual and physical addresses
  - since each page is 2^6 = 64 bytes, the low order 6 bits of the virtual and physical addresses serve as the VPO
    and PPO respectively
  - the high-order 8 bits of the virtual address serve as the VPN
  - the high order 6 bits of the physical address serve as the PPN

Figure 9.20 shows a snapshot of our little memory syustem, including the TLB (Figure 9.20(a), a portion of the page
table (Figue 9.20(b)), and the L1 cache (Figure 9.20(c))
  - above the figures of the TLB and cache, we have also shown how the bits of the virtual and physical addresses are
    partitioned by the hardware as it accesses these devices
      * TLB:  The TLB is virtually addressed using the bits of the VPN. Since the TLB has four sets, the 2 low-order
	      bits of the VPN serve as the set index (TLBI). The remaining 6 high-order bits serve as the tag (TLBT)
	      that distinguishes the different VPNs that might map to the same TLB set.
      * Page table:   The page table is a single level design with a total of 2^8 = 256 page table entries (PTEs).
		      However, we are only interested in the first sixteen of these. For convenience, we have labled
		      each PTE with the VPN that indexes it; but keep in mind that these VPNs are not part of the
		      page table and not stored in memory. Also notice that the PPN of each invalid PTE is denoted
		      with a dash to reinforce the idea that whatever bit values might happen to be stored there
		      are not meaningful.
      *	Cache:	The direct mapped cache is addressed by the fields in the physical address. Since each block is 4
		bytes, the low order 2 bits of the physical address server as the block offset (CO). Since there
		are 16 sets, the next 4 bits server as the set index (CI). THe remaining 6 bits serve as the tag (CT)

Given this initial setup, let's see that happens when the CPU executes a load instruction that reads the byte at
address 0x03d4 (recall that our hypothetical CPU reads one-byte words rather than four-byte words)
  - to begin this kind of manual simulation, we find it helful to write down the bits in the virtual address,
    identify the various fields we will need, and determine their hex values.
  - the hardware performs a similar task when it decodes the address
  - to begin the MMU extracts the VPN (0x0F) from the virtual address and checks with the TLB to see if it has
    cached a copy of the PTE 0x0F from some previous memory reference.
  - the TLB extracts the TLB index (0x03) and the TLB tag (0x3) from the VPN, hits on a valid match in the second
    entry of Set 0x3, and returns the cached PPN (0X0D) to the MMU
  - if the TLB had missed, then the MMU would need to fetch the PTE from main memory
  - however, in this case we got lucky and had a TLB hit, the MMU now has everything it needs to form the physical
    address
  - it does this by concatenating the PPN (0x0D) from the PTE with the VPO (0x14) from the virtual address, which
    forms the physical address (0x354)
  - next the MMU sends the physical address to the cache, which extracts the cache offset CO (0x0), the cache set
    index CI (0x5), and the cache tag CT (0x0D) from the physical address
  - since the tag in set 0x5 mathces CT, the cache detects a hit, reads out the data byte (0x36) at offset CO, and
    returns it to the MMU, which then passes it back to the CPU
  - other paths throught the translation process are possible.
  - for example, if the TLB misses, then the MMU must fetch the PPN from a PTE in the page table
  - if the resulting PTE is invlaid, then there is a page fault and the kernel must page in the appropriate page and
    return the load instruction
  - another possibility is that the PTE is valid, but the necessary memory block misses in the cache

Memory Mapping

Linux (along with other forms of Unix) initializes the contents of a virtual memory area by associating it with an
object on disk, a process known as memory mapping
  - areas can be mapped to one of two types of objects:
      1.  regular file in the Unix file system:	an area can be mapped to a contiguous section of a regular disk file,
	  such as an executable object file. The file section is divided into page-sized pieces, with each piece
	  containing the initial contents of a virtual page. B/c of demand paging, none of these virtual pages is
	  actually swapped into physical memory until the CPU first touches the page (i.e., issues a virtual address
	  that falls with that page's region of the address space). If the area is larger than the file section, then
	  the area is padded with zeros
      2.  anonymous file: an area can also be mapped to an anonymous file, created by the kernel, that contains all
	  binary zeros. The first time the CPU touches a virtual page in such an area, the kernel finds an
	  appropriate victim page in physical memory, swaps out the victim page if it is dirty, overwrites the
	  victim page with binary zeros, and updates the page table to mark the page as resident. Notice that no
	  data is actually transferred between disk and memory. For this reson, pages in areas that are mapped to
	  anonymous files are sometimes called demand-zero pages.

In either case, once a virtual page is initialized, it is swapped back and forth between a special swap file
maintained by the kernel.
  - the swap file is also known as the swap space or the swap area
  - an important point to realize is that at any point in time, the swap space bounds the total amount of virtual
    pages that can be allocated by the currently running processes

Shared Objects Revisited

The idea of memory mapping resulted from a clever insight that if the virtual memory system could be integrated into
the conventional file system, then it could provide a simple and efficient way to load programs and data into mem-
ory

As we have seen, the process abstraction promises to provide each process with its own private virtual address space
that is protected from errant writes or reads by other processes
  - however, many processes have identical read-only text areas
  - for example, each process that runs the Unix shell program tcsh has the same text area
  - further, many programs need to access identical copies of read-only run-time library code
      *	every C program requires function from the standard C library such as printf
  - it would be extremely wasteful for each process to keep duplicate copies of these commonly used codes in physical
    memory
  - fortunately, memory mapping provides us with a clean mechanism for controlling how object are shared by multiple
    processes

On object can be mapped into an area of virtual memory as either a shared object or a private object
  - if a process maps a shared object into an area of its vitual address space, then any writes that the process
    makes to that area are visible to any other processes that have also mapped the shared object in their virtual
    memory
  - further, the changes are also reflected in the original object on disk

Changes made to an area mapped to a private object, on the other hand, are not visible to other processes, and any
writes that the process makes to the area are not reflected back to the obejct on disk
  - a virtual memory area into which a shared object is mapped is often called a shared area
  - similarly for a private area

Suppose process 1 maps a shared object into an area of its virtual memory, as shown in Figure 9.29(a)
  - suppose that process maps the same shared object into its address space (not necessarily at the same virtual
    address as process 1), as shown in Figure 9.29(b)

Since each object has a unique file name, the kernel can quickly determine that process 1 has already mapped this
object and can point the page table entries in process 2 to the appropriate physical pages
  - the key point is that only a single copy of the shared object needs to be stored in physical memory, even though
    the object is mapped into multiple shared areas
  - for convenience, we have shown the physical pages as being contiguous, but of course this is not true in general

Private objects are mapped into virtual memory using a clever technique known as copy-on-wrtie
  - a private object begins line in exactly the same way as a shared object, with only one copy of the provate object
    stored in physical memory
  - for example Figure 9.30(a) shows a case where two processes have mapped a private object into different of their
    virtual memories but share the same physical copy of the object
  - for each process that maps the private object, the page table entries for the corresponding priavte area are
    flagged as read-only, and the struct is flagged as private copy-on-write
  - so long as neither process attempts to write to its respective private area, they continue to share a single
    copy of the object in physical memory
  - however as soon as a process attempts to wirte to some page in the private area, the write triggers a
    protection fault

When the fault handler notices that the protection exception was caused by the process trying to write to a page in
a private copy-on-write area, it creates a new copy of the page in physical memory, updates the page table entry to
pont ot the new copy, and then restores write permissions to the page, as whown in Figure 9.30(b)
  - when the fault handler returns, the CPU reexecutes the wirte, which now proceeds normally on the newly created
    page

By defering the copying of the pages in private object until the last possible moment, copy-on-write makes the most
efficient use of scarce physical memory

The fork Function Revisisted

Now that we understnad virtual memory and memory mapping, we can get a clear idea of how the fork function creates a
new process with its own independent virtual address space

When the fork function is called by the current process, the kernel creates various data structures for the new
process and assigns it a unique PID
  - to create the virtual memory for the new process, it creates exact copies of the current process's mm_struct,
    area structs, and page tables
  - it flags each page in both processes as read-only, and flags each area struct in both process as private copy-on-
    write

When the fork returns in the new process, the new process now has an exact copy of the virtual memory as it existed
when the fork was called
  - when either of the processes performs any subsequent writes, the copy-on-write mechanism creates new pages, thus
    preserving the abstraction of a private address space for each process

The execve Function Revisited

Virtual memory and memory mapping also play key roles in the process of loading programs into memory
  - now that we understand these concepts, we can understand how the execve function really loads and executes
    programs
  - suppose that the program running in the current process makes the following call:

	Execve("a.out", NULL, NULL);

  - as you learned in chapter 8, the execve function loads and runs the program contained in the executable object
    file a.out within the current process, effectively replacing the current program with the a.out program
  - loading and running a.out requires the following steps:
      *	delete existing user areas: delete the existing area structs in the user portion of the current process's
	virtual address
      *	map private areas: create new area structs for the text, data, bss, and stack areas of the new program. All
	of these new areas are private copy-on-write. The text and data areas are mapped to the text and data
	sections of the a.out file. The bss area is demand-zero, mapped to an anonymous file whose size is contained
	in a.out. The stack and heap area are also demand-zero, initially of zero length. Figure 9.31 summarizes the
	different mappings of the private areas
      *	map shared areas: if the a.out program was linked with shared objects, such as the standard C library libc.so,
	then these objects are dynamically linked into the program, and then mapped into the shared region of the
	user's virtual address space
      *	set the program counter (PC): the last thing that execve does is set the program counter in the current
	processes context to point to the entry point in the text area

The next time this process is scheduled, it will begin execution from the entry point
  - Linux will swap in code and data pages as needed

User-level Memory Mapping with the mmapp Function

Unix processes can use the mmap function to create new areas of virtual memory and to map objects into these areas

  #include <unistd.h>
  #include <sys/mman.h>

  void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);  //  Returns: pointer to mapped
										      //  area if OK, MAP_FAILED (-1)
										      //  on error

The mmap function asks the kernel to create a new virtual memory area, preferably one that starts at address start,
and to map a contiguous chunck of the object specified by file descriptor fd to the new area
  - the contiguous object chunk has a size of length bytes and starts at an offset of offset bytes from the beginning
    of the file
  - the start address is merely a hint, and is usually specified as NULL
  - for our purposes, we will always assume a NULL start address, figure 9.32 depicts the meaning of these arguments

The prot argument contains bits that describe the access permission of the newly mapped virtual memory area (i.e.,
the vm_prot bits in the corresponding area struct)
  - PROT_EXEC: pages in the area consist of instructions that may be executed by the CPU
  - PROT_READ: pages in the area may be read
  - PROT_WRITE:	pages in the area may be written
  - PROT_NONE: pages in the area cannot be accessed

The flags argument consists of bits that describe the type of the mapped object
  - if the MAP_ANON flag bit is set, then the backing store is an anonymous object and the corresponding virtual
    pages are demand-zero
  - MAP_PRIVATE indivates a private copy-on-write object, and MAP_SHARED indicates a shared object

      bufp = Mmap(-1, size, PROT_READ, MAP_PRIVATE|MAP_ANON, 0, 0)

  - the above function asks the kernel to create a new read-onle, private, demand zero area of virtual memory
    containing size bytes
  - if the above call is successful, then bufp contains the address of the new area

The munmap function deletes regions of virtual memory:

  #include <unistd.h>
  #incude <sys/mman.h>

  int munmap(void *start, size_t length);     //  Returns: 0 if OK, -1 on error

  - the munmap function deletes the area starting at virtual address start and consisting of the next length bytes
  - subsequent references to the deleted region result in segmentation faults

Dynamic Memory Allocation

While it is certainly possible to use the low-level mmap and munmap functions to create and delete areas of virtual
memory, C programmers typically find it more convenient and more portable to use a dynamic memory allocator when they
need to acquire additional virtual memory at run time.

A dynamic memory allocator maintains an area of a process's virtual memory known as the heap (see figure 9.33)
  - details vary from system to system, but without loss of generality, we will assume that the heap is an area of
    demand-zero memory that beging immediately after the uninitialized bss area and grows upward (toward higher
    addresses)
  - for each process, the kernel maintains a variable brk (pronounced "break") that points to the top of the heap

An allocator maintains the heap as a collection of various-sized blocks
  - each block is a contiguous chunk of virtual memory that is either allocated or free
  - an allocated block has been explicitly reserved for use by the application
  - a free block is available to be allocated
  - a free block remains free until it is explicitly allocated by the application
  - an allocated block remain allocated until it is free, either explicitly by the application, or implicitly by
    the memory allocator itself

Allocators come in two basic styles, both styles require the application to explicitly allocate blocks
  - they differ about which entity is responsible for free allocated blocks
      * explicit allocators require the application to explicitly free any allocated blocks. For example, the C
	standard library provides an explicit allocator called malloc package. C programs allocate a block by
	calling the malloc function, and free a block by calling the free function. The new and delete calls in
	C++ are comparable
      *	implicit allocators, on the other hand, require the allocator to detect when an allocated block is no longer
	being used by the program and then free the block. Implicit allocators are also known as garbage collectors,
	and the process of automatically freeing unused allocated blocks is known as garbage collection. For example,
	higher-level languages such as Lisp, ML, and Java rely on garbage collection to free allocated blocks

The remainder of this section discusses the design and implementation of explicit allocators
  - we will discuss implicit allocators in Section 9.10
  - for concreteness, out discussion focuses on allocators that manage heap memory
  - however, you should be awae that memory allocation is a general idea that arises in a variety of contexts
  - for example, application that do intensive manipulation of graphs will often use the standard allocator to
    acquire a large block of virtual memory, and then use an application-specific allocator to manage the
    memory within that block as the nodes of the graph are created and destroyed

The malloc and free Functions

The C standard library provides an explicit allocator known as the malloc package
  - programs allocate blocks from the heap by calling the malloc function

	#include <stdlib.h>

	void *malloc( size_t size );	//  Returns: ptr to allocated block if OK, NULL on error

The malloc function returns a pointer to a block of memory of at least size bytes that is suitably aligned for any
kind of data object that might be contained in that block
  - on Unix systems that we are familiar with, malloc returns a block that is aligned to an 8-byte (double word)
    boundary

How big is a word?

Recal from our discussion of machine code in Chapter 3 that Intel refers to 4-byte objects as double words
  - however, throughout this section, we will assume that words are 4-byte objects and that double words are 8-byte
    objects, which is consistent with conventional terminology

If malloc encounters a problem (e.g., the program requests a block of memory that is larger than the available
virtual memory), then it returns NULL and sets erno.
  - Malloc does not initialize the memory it returns
  - application that want initialized dynamic memory can use calloc, a thin wrapper around malloc function that
    initializes the allocated memory to zero
  - application that want to change the size of a previously allocated block can use realloc function

Dynamic memory allocators such as malloc can allocate or deallocate heap memory explicitly by using the mmap and
munmap functions, or they can use the sbrk function

      #include <unistd.h>

      void *sbrk(intptr_t incr);    //	Returns: old brk pointer on success, -1 on error

The sbrk function grows or shrinks the heap by adding incr to the kernel's brk pointer
  - if successful, it returns the old value of brk, otherwise it returns -1 and sets errno to ENOMEM
  - if incr is zero, the sbrk returns the current value of brk
  - calling sbrk with a negative incr is legal but tricky because the reutrn value (the old value of brk) points to
    abs(incr) bytes past the new top of the heap

Program free allocated heap blocks by calling the free function

    #include <stdlib.h>

    void free( void *ptr );

The ptr argument must point to the beginning of an allocated block that was obtained from malloc, calloc, or realloc
  - if not, then the behavior of free is undefined
  - even worse, since it returns nothing, free gives no indication to the applcation that something ir wrong
  - as we shall see in section 9.11, this can produce some baffling run-time errors

Figure 9.34 shows how an implementation of malloc and free might manage a very small heap of 16 words for a C
program
  - each box represents a 4-byte word
  - the heavy lined rectangles correspond to allocated blocks (shaded) and free blocks (unshaded)
  - initially, the heap consists of a single 16-word double-word aligned free block
      * figure 9.34 (a): the program asks for a four word block. Malloc responds by carving out a four word block
	from the fron of the free block and returning a pointer to the first word of the block
      *	figure 9.34 (b): the program requests a five word block. Malloc responds by allocating a six word block from
	the front of the free block. In this example, malloc pads the block with a extrea word in order to keep the
	free block aligned on a double word boundary
      *	figure 9.34 (c): the program requests a siz word block and malloc responds by carving out a six word block
	from the free block
      *	figure 9.34 (d): the program frees the six word block that was allocated in figure 9.34 (b). Notice that
	after the call to free returns, the pointer p2 still points to the freed block. It is the respondibility of
	the applicaiton not to use p2 again until it is reinitialized by a new call to malloc
      *	figure 9.34 (e): the program requests a two-word block. In this case, malloc allocates a portion of the block
	that was freed in the previous step and returns a pointer to the new block

Why Dynamic Memory Allocation?

The most important reason why program use dynamic memory allocation is that often they do not know the sizes of
certain data structures until the program actually runs
  - for example, suppose that we are asked to write a C program that reads a list of n ASCII integers, one integer
    per line, from stdin into a c array
  - the input consists of the integer n, followed by the n integers to be read and stored into the array
  - the simplest approach is to define the array statically with some hard-coded maximum array size:
  - allocating arrays with hard coded sizes like this is often a bad idea
  - the value of MAXN is arbitrary and has no relation to the actual amount of available virtual memroy on the
    machine
  - further, if the user of this program wanted to read a file that was karger than MAXN, the only recourse would be
    to recompile the program with a larger value of MAXN.
  - while not a problem for this simple example, the presence of hard-coded array bounds can become a maintenance
    nightmare for large software products with millions of lines of code and numerous users

A better approach is to allocate the array dynamically, at run time, after the value of n becomes known
  - with this approach, the maximum size of the array is limited only by the amoutn of available virtual memory
  - dynamic memory allocation is a useful and important programming technique
  - however, in order to use allocators correctly and efficiently, programmers need to have an understanding of how
    they work
  - we will discuss some of the gruesome error that can result from the improper use of allocators in section 9.11

Allocator Requirements and Goals

Explicit allocators must operate within som stringent constraints
  - handling arbitrary request sequences: an applicaiton can make an arbitrary sequence of allocate and free requests,
    subject to the constraint that each free request must correspond to a currently allocated block obtained from a
    previous allocate request. Thus, the allocator cannot make any assumptions about the ordering of allocate and free
    requests. For example, the allocator cannot assume that all allocate requests are accompanied by a matching free
    request, or that matching allocate and free requests are nested.
  - making immediate responses to requests: the allocator must respond immediately to allocate requests. Thus, the
    allocator is not allowed to reorder or buffer requests in order to improve performance
  - using only the heap: in order for the allocator to be scalable, and non-scalar data structures used by the
    allocator must be stored in the heap itself. Aligning blocks (alignment requirement). The allocator must align
    blocks in such a way that they can hold any type of data object. On most systems, this means that the block
    returned by the allocator is aligned on an 8-byte (double-word) boudary
  - not modifying allocated blocks: allocators can only manipulate or change free blocks. In particular, they are not
    allowed to modify or move blocks once they are allocated. Thus, techniques such as compaction of allocated blocks
    are not permitted

Working within these constraints, the author of an allocator attempts to meet the often conflicting performance goals
of maximizing throughput and memory utilization:
  - Goal 1: maximizing throughput. Given some sequence of n allocate and free requests

	R_0, R_1, ... , R_k, ... , R_(n - 1)

    we would like to maximize an allocator's throughput, which is defined as the number of requests that it completes
    per unit time. For example, if and allocator complete 500 allocate requests and 500 free requests in 1 second,
    then its throughput is 1,000 operations per second. In general, we can maximize throughput by minimzing the
    average time to satisfy allocate and free requests. As we'll see, it is not too dificult to develop allocatros
    with reasonably good performance where the worst-case running time of an allocate request is linear in the number
    of free blocks and the running time of a free request is constant
  - Goal 2: mazimizing memory utilization. Naive programmers often incorrectly assume that virtual memory is an
    unlimited resource. In fact, the total virtual memory allocated by all of the processes in a system is limited by
    the amount of swap space on disk. Good programmers know that virtual memory is a finite resource that must be
    used efficiently. This is especially true for a dynamic memory allocator that might be asked to allocate and free
    large blocks of memory

There are a number of ways to characterize how efficiently an allocator uses the heap
  - in our experience, the most useful metric is peak utilization
  - before we are given some sequence of n allocate and free requests

	R_0, R_1, ... , R_k, ... , R_(n - 1)
  
  - if an applicaiton requests a block of p bytes, then the resulting allocated block has a payload of p bytes
  - after request R_k has completed, let the aggregate payload, denoted by P_k, be the sume of the payloads of the
    currently allocated blocks, and H_k denote the current (monotomically nondecreasing) size of the heap
  - then the peak utilization over the first k requests, denoted by U_k, is given by:

	  U_k = (max_(i <= k) P_i)/H_k

  - The objective of the allocator then is to maximize the peak utilization U_(n-1) over the entire sequence
  - As we will see, there is a tension between maximizing throughput and utilization
  - in particular, it is easy to write an allocator that maximizes throguhput at the expense of heap utilization.
  - one of the interesting challenges in any allocator design is finding an appropriate balance between the two goals

Relaxing the monotonicity assumption

  - we could relax the monotonically nondecreasing assumption in our definition of U_k and allow the heap to grow
    up and down by letting H_k be the high water marek over the first k requests

Fragmentation

The primary cause of poor heap utilization is a phenomenon known as fragmentation, which occurs when otherwise unused
memory is not available to satisfy allocate requests
  - there are two forms of fragmentation: internal fragmentation and external fragmentation

Internal fragmentation occurs when an allocated block is larger than the pay load
  - this might happen for a number of reasons
  - for example, the implementation of an allocator might impose a minimum size on allocated blocks that is greater
    than some requested payload
  - or as we saw in figure 9.34 (b) the allocator might increas the block size in order to satisfy alignment
    constraints
  - internal fragmentation is straighforward to quantify, it is simply the sum of the differences between the sizes
    of the allocated blocks and their payloads
  - thus at any point in time, the amount of internal fragmentaion depends only on the pattern of previous requests
    and the allocator implementation

External fragmentation occurs when there is enough aggregate free memory to satisfy an allocate request, but no
single free block is large enough to handle the request
  - for example, if the request in figure 9.34 (e) were for size words wather than two words, then the request could
    not be satisfied without requesting additional virtual memory from the kernel, even though there are six free
    words remaining in the heap
  - the problem arises because these size words are spread over two free blocks
  - external fragmentaion is much more difficult to quantify than internal fragmentation because it depends not only
    on the pattern of previous requests and the allocator implementation, but also on the pattern of future requests
  - for example, suppose that after k requests all of the free blocks are exactly four words in size
  - does this heap suffer from external fragmentation
  - the answer depends on the pattern of future requests
  - if all of the future allocate requests are for blocks that are smaller than or equal to four words, then there is
    no external fragmentation
  - on the other hand, if one or more requests ask for blocks larger than four words, then the heap does suffer from
    external fragmentation
  - since external fragmentation is difficult to quantify and impossible to predict, allocators typically employ
    heuristics that attempt to maintain small numbers of larger free blocks rather than large numbers of smaller free
    blocks

Implementation Issues

The simplest imaginable allocator would organize the heap as a large array of bytes and a pointer p that initially
points to the first byte of the array
  - to allocate size bytes, malloc would save the current value of p on the stack, increment p by size, and return
    the old value of p to the caller
  - Free would simply return to the caller without doing anything

This naive allocator is an extreme point in the design space
  - since each malloc and free execute only a handful of instructions, throughput would be extremely good
  - however, since the allocator never reuses any blocks, memory utilization would be extremely bad
  - a practical allocator that strikes a better balance between throughput and utilization must consider the
    following issues:
      * free block organization: how do we keep track of free blocks?
      *	placement: how do we choose an appropriate free block in which to place a newly allocated block?
      *	splitting: after we place a newly allocated block in some free block, what do we do with the remainder of the
	free block?
      *	coalescing: what do we do with a block that has just been freed?

The rest of this section looks at these issues in more detail
  - since the basic techniques of placement, splitting, and coalescing cut accross many different free block
    organizations, we will introduce them in the context of a simple free block organzation known as implicit free
    list

Implicit Free Lists

Any practical allocator needs some data structure that allows it to distinguish block boundaries and the distinguish
between allocated and free blocks
  - most allocators embed this information in the blocks themselves, a simple approach is shown in figure 9.35
  - in this case, a block consists of a one-word header, the payload, and possible some additional padding
  - the header encodes the block size (including the header and any padding) as well as whether the block is allocated
    or free
  - if we impose a double-word alignment constriant, then the block size is always a multiple of eight and the 3 low-
    order bits of the block size are always zero
  - thus, we need to store aonly the 29 high-order bits of the block size, freeing the remaining 3 bits to encode
    other information
  - in this case, we are using the least significant of these bits (the allocated bit) to indicate whether the block
    is allocated or free
  - for example, suppse we have an allocated block with a block size of 24 (0x18) bytes, then its header would be

	0x00000018 | 0x1 = 0x00000019

  - similarly, a free block with a block size of 40 (0x28) bytes would have a header of

	0x00000028 | 0x0 = 0x00000028

The header is followed by the payload that the application requested when it called malloc
  - the payload is follwed by a chunk of unused padding that can be any size
  - there are a number of reasons for padding
      * for exmaple, the padding might be part of an allocator's strategy for combating external fragmentation
      * or it might be needed to satisfy the alignment requirement
  - given the block format in figure 9.35, we can organize the heap as a sequence of contiguous allocated and free
    blocks, as shown in figure 9.36
  - we call this organization an implicit free list becasue the free blocks are linked implicitly by the size fields
    in the headers
  - the allocator can indirectly traverse the entire set of free blocks by traversing all of the blocks in the heap
  - notice that we need some kinf of specially makred en block, in this example a terminating header with the
    allocated bit sety and a size of zero.
  - as we will see in section 9.9.12, setting the allocated but simplifies the coalescing of free blocks

The advantage of an implicit free list is simplicity, a significant disadvantage is that the cost of any operation,
such as placing allocated blcos, that requires a search of the free list will be linear in the total number of
allocated and free blocks in the heap
  - it is important to realize that the system's alignment requirement and the allocator's choice of block format
    impose a minimum block size on the allocator
  - no allocated or free block may be smaller than this minimum
  - for example, if we assume a double word alignemnt requirement, then the size of each block must be a multiple of
    two words (8 bytes)
  - thus the block format in Figure 9.35 induces a minimum block size of two words:
      * one word for the header
      * another word to maintain the alignment requirement
  - even if the application were to request a single byte, the allocator would still create a two word block

Placing Allocated Blocks

When an application requests a block of k bytes, the allocator searches the free list for a free block that is large
enough to hold the requested block
  - the manner in which the allocator performs this search is determined by the placement policy
  - some common policies are first fit, next fit, and best fit

First fit searches the free list from the beginning and chooses the first free block that fits

Next fit is similar to first fit, but instead of starting each search at the beginning of the list, it starts
each search where the previous search let off

Best fit examines every free block and chooses the free block with the smallest size that fits

An advantage of first fit is that it tends to retain free blocks at the end of the list
  - a disadvantage is that it tends to leave "plinters" of small free blocks toward the beginning of the list,
    which will increase the search time for larger blocks

Next fit was first proposed by Donald Knuth as an alternative to first fit, motivated by the idea that if we found a
fit in some free block the lasttime, there is a good chance the we will find a fit the next time in the remainder of
that block
  - next fit can run significantly faster than first fit, especially if the front of the list becomes littered with
    many small splinters
  - however, some studies suggest that next fit suffers from worse memory utilization than first fit

Studies have found that best fit generally enjoys better memory memory ultilization than first fit or next fit
  - however, the disadvantage of useing best fit with dimple free list organizations such as the implicit free
    list, is that it requires an exhaustive search of the heap
  - later, we will loop at more sophisticated segrated free list organizations that approximate a best-fit policy
    without an exhaustive search of the heap

Splitting Free Blocks

Once the allocator has located a free block that fits, it must make another poicy decision about how much of the
free block to allocate
  - one option is to use the entire free block
  - although simple and fast, the main disadvantage is that it introduces internal fragmentation
  - if the placement policy tents to produce food fits, then some additional internal fragmentation might be
    acceptable
  - however, if the fit is not good, then the allocator will usually opt to spliut the free block into two parts
  - the first part becomes the allocated block, and the remainder becomes a new free block
  - figure 9.37 shows how the allocator might split the wight word free block in figure 9.36 to satisfy an
    application's request for three words of heap memory

Getting Additional Heap Memory

What happens if the allocator is unable to find a fir for the requested block?
  - one option is to try to create some larger free blocks by mergin (coalescing) free blocks that are physically
    adjacent in memory
  - however, if this does not yield a sifficiently large block, or if the free blocks are already maximally
    coalesced, then the allocator asks the kernel for additional heap memory by called the sbrk function
  - the allocator transforms the additional memory into onlarge free block, inserts the block into the free list,
    and then places the requested block in this new free block

Coalescing Free Blocks

When the allocator frees an allocated block, there might be other free blocks that are adjacent to the newly free
block
  - such adjacent free blocks can cause a phenomenon known as false fragmentation, where there is a lot of available
    free memry chopped up into small, unusable free blocks
  - firgure 9.38 shows the reulst of free the block that was allocated in figure 9.37
  - the result is two adjacent free blocks with payloads of three words each
  - as a result a subsequent request for a payload of four words would fail, even though the aggregate size of the
    two free blocks is large enough to satisfy the request

To combat false fragmentation, any practical allocator must merge adjacent free blocks in a process known as
coalescing
  - this raises an important polucy decision about when to perform coalescing
  - the allocatorcan opt for immediate coalescing by mergin any adjacent blocks each time a block is freed
  - or it can opt for deferred coalescing by waiting to coalesce free blocks at some later time
  - for example, the allocator might defer coalescing until some allocation reuest fails, and then scan the entire
    heap, coalescing free blocks

Immediate coalescing is straightforward and can be performed in constant time, but with some request patterns it can
introduce a form of thrashing where a block is repeatedly coalesced and then split soon after
  - in figrue 9.38 a repeated pattern of allocating and freeing a three word block would introduce a lot of 
    unnecessary splitting and coalescing
  - in our discussion of allocators, we will assume immediate coalescing, but you should be aware that fast
    allocators often opt for some form of deferred coalescing

Coalescing with Boundary Tags

How does an allocator implement coalescing?
  - let us refer to the block we want to free as the current block
  - then coalescing the next free block (in memory) is straightforward and efficient
  - the header of the current block points to the header of the next block, which can be checked to determine if the
    next block is free
  - if so, its size is simply added to the size of the current header and the blocks are coalesced in constant time

But how would we coalesce the previous block?
  - given an implicit free list of blocks with headers, the only option would be to search the entire list,
    remembering the location of the previous block, until we reached the current block
  - with an implicit free list, this means that each call to free would require time linear in the size of the heap
  - even with more sophisticated free list organizations, the search time would not be constant

Knuth developed a clever and general technique, known as boundary tags, that allows for constant time coalescing of
the previous block
  - the idea, which is shown in figure 9.39 is to add a footer (the boundary tag) at the end of each block, where
    the footer is a replica of the header
  - if each block includes such a footer, then the allocator can determine the starting location and status of the
    previous block by inspecting its footer, which is always one word away fomr the start of the current block

Consider all the cases that can exist when the allocator frees the current block
  1.  the previous and next blocks are both allocated
  2.  the previous block is allocated and the next block is free
  3.  the previous block is free and the next block is allocated
  4.  the previous and next blocks are both free

Figure 9.40 shows how we would coalesce each of the four cases
  - in case 1, both adjacent blocks are allocated and thus no coalescing is possible, so the status of the current
    block is simply changed from allocated to free
  - in case 2, the current block is merged with the next block, the header of the current block and the footer of
    the next next block are updated with the combined sizes of the current and next blocks
  - in case 3, the previous block is merged with the current block, the header of the previous block and the footer
    of the current block are updated with the combined sizes of the two blocks
  - in case 4, all three blocks are merged to form a single free block, with the header of the previous block and
    and the footer of the next block updated with the combined sizes of the three blocks
  - in each case, the coalescing is performed in constant time

The idea of boundary tags is a simple and elegant one that generalizes to many different types of allocators and
free list organizations
  - however, there is a potential disadvantage
  - requiring each block to contain both a header and a footer can introduce significant memory overhead if an
    application manipulates many small blocks
  - for example, if a graph application dynamically creates and destroys graph nodes by making repeated calls to
    malloc and free, and each graph node requires only a couple of words of memory, then the header and the footer
    will consume half of each allocated block

Fortunately there is a clever optimization of boundary tags that eliminates the need for a footer in allocated
blocks
  - recall that when we attempt to coalesce the current block with the previous and next blocks in memory, the size
    field in the footer of the previous block is only needed is the previous block is free
  - if we were to store the allocated/free bit of the previous block in one of the excess low-order bits of the
    current block, then allocated blocks would not need footers, and we could use that extra space for payload
  - note that free blocks would still need footers

Putting It Together: Implementing A Simple Allocator

Building an allocator is a challenging task
  - the design space is large, with numerous alternatives for block format and free list format, as well as place-
    ment, splitting, and coalescing policies
  - another challenge is that you are often forced to prgram outside the safe, familiar confines of the type sytem,
    relying on the error-prone pointer casting and pointer arithmetic that is typical of low-level systems programm-
    ing

While allocators do not requires enourmous amounts of code, they are subtle and unforgiving
  - students familiar with higher-level languages such as C++ or Java often hit a conceptual wall when they first
    encounter this style of programming
  - to help you clea this hurdle, we will work through the implementation of a simple allocator based on an implicit
    free list with immediate boundary-tag coalescing
  - the maximum block size is 2^32 = 4 GB
  - the code is 64-bit clean, running without modification in 32-bit (gcc -m32) or 64-bit (gcc -m64) processes

General Allocator Design

Our allocator uses a model of the memory system porivded by the memlib.c package shown in figure 9.41
  - the purpose fo the model is to allow us to run our allocator without interfering with the existing system-level
    malloc package
  - the mem_init function models the virtual memory available to the heap as a large, double-word aligned array of
    bytes
  - the bytes between mem-heap and mem_brk represent allocated virtual memory
  - the bytes following mem_brk represent unallocated virtual memory
  - the allocator requests additional heap memory by called in the mem_sbrk function, which has the same interface
    as the system's sbrk function, as well as the sematics, except that it rejects requests to shrink theheap

The allocator itself is contained in a source file (mm.c) that users can compile and link into their applications
  - the allocator exprts three function to application programs:
      1.  extern int mm_init(void);
      2.  extern void *mm_malloc(size_t size);
      3.  extern void mm_free(void *ptr);
  - the mm_init function intialized the allocator, returning 0 if successful and -1 otherwise
  - the mm_malloc and mm_free functions have the same interfaces and sematics as their system counterparts
  - the allocator uses the block format shown in figure 9.39
  - the minimum block size is 16 bytes
  - the free list is organized as an implicit free list, with the invariant form shown in figure 9.42

The first word is an unused padding word aligned to a double-word boundary
  - the padding is followed by a special prologue block, which is an 8 byte allocated block consisting of only a
    header and a footer
  - the prologur block is created during initialization and is never freed
  - following the prologue block are zero or more regular blocks that are created by calls to malloc and free
  - the heap always ends with a special epilogue block, which is a zero sized allocated block that consists of
    a header
  - the prologue and epilogue blocks are tricks that eliminate the edge conditions during coalescing
  - the allocator uses a single private static global vairbale (heap_listp) that always points to the prologue
    block
  - as a minor optimization, we could make it a pointer to the next block instead of the prologue block

Basic Constants and Macros for Manipulating the Free List

Figure 9.43 shows some basic constants and macros that we will use throughout the allocator code
  - lines 2-4 define some basic size constants:
      * the sizes of words (WSIZE)
      * double words (DSZIE)
      * and the size of the initial free block and the default size for expanding the heap (CHUNKSIZE)

Manipulating the headers and footers in the free list can be troublesome because it demands extensive use of casting
and pointer arithmetic
  - thus we find it helpful to define a small set of macros for accessing and traversing the free list (lines 9-25)
  - the PACK macro (line 9) combines a size and an allocate bit and returns a value that can be stored in a header
    or footer
  - the GET macro (line 12) reads and returns the word referenced by argument p
      *	the casting here is crucial
      *	the argument p is typically a (void *) pointer, which cannot be derefernced directly
  - the PUT macro (line 13) staores val in the word pointed at by argument p
  - the GET_SIZE and GET_ALLOC macros (lines 16-17) return the size and allocated bit, respectively, from a header
    or footer at address p
  - the remaining macros operate on block pointer (denoted bp) that point to the first payload byte
  - given a block pointer bp, the HDRP and FTRP macros (lines 20-21) return pointers to the block header and footer,
    respectively
  - the NEXT_BLKP and PREV_BLKP macor (lines 24-25) return the block pointer of the next and previous blocks,
    respecitvely

The macros can be composed in various ways to manipulate the free list
  - for example given a pointer bp to the current block, we could use the following line of code to determine the
    size of the next block in memory:

      size_t size = GET_SIZE(HDRP(NEXT_BLKP(bp)));

Creating the Initial Free List

Before calling mm_malloc or mm_free, the application must initialize the heap by calling the mm_init function,
figure 9.44
  - the mm_init function gets four words from the memory system and initializes them to create the empty free list
    (lines 4-10)
  - it then calls the extend heap function, figure 9.45, which extends the heap by CHUNKSIZE bytes and creates the
    initial free block
  - at this point, the allocator is initialized and ready to accept, allocate and free requests from the application

The extend_heap function is onvoked in two different circumstances:
    1.  when the heap is initialized
    2.  then mm_malloc is unable to fund a suitable fit
  - to maintain alignment, extend_heap rounds up the requested size to the nearest multiple of 2 words (8 bytes),
    then requests the additional heap space from the memory system (lines 7-9)
  - the remainder of the extend_heap function (lines 12-17) is somewhat subtle
  - the heap begins on a double-word aligned boundary, and every call to extend_heap returns a block whose size is
    an integral number of double words
  - thus, every call to mem_srbk returns a double-word aligned chunk of memory immediately following the header of
    the epilogue block
  - this header becoes the header of the new free block (line 12), and the last word of the chunk becomes the new
    epilogue block header (line 14)
  - finally, in the likely case that the previous heap was terminated by a free a free block, we call the coalesce
    function to mege the two free blocks and return the block pointer of the merged blocks (line 17)

Freeing and Coalesvcing Blocks

An application frees a previously allocated block by calling the mm_free function (figure 9.46), which frees the
requested block (bp) and the nmerges adjacent blocks using the boundary tags coalesing techniques described in
section 9.9.11

The code in the coalesce helper function is a straighforward implementation of the four cases outlines in figure
9.40
  - there is on somewhat subtle aspect
  - the free list format we have chosen - with its prologue and epilogue blocks that are alwys maked as allocated -
    allows us to ignore the potentially troublesome edge conditions where the requested block bp is at the beginning
    or end of the heap
  - without these special blocks, the code would be messier, more error prone, and slower, b/c we would have to
    to check for there rare edge conditions on each and every free request

Allocating Blocks

An application requests a block of size bytes of memory by calling the mm_malloc fucntion (figure 9.47)
  - after checking for spurious requests, the allocator must adjust the requested block size to allow room for the
    header and footer, and to satisfy the double word alignment requirement
  - lines 12-13 enforce the minimum block size of 16 bytes: 8 bytes to satisfy the alignment requirement, and 8
    more for the overhead of the header and footer
  - for requests over 8 bytes (line 15), the general rule is to add in the overhead bytes and then round up to the
    nearest multiple of 8

Once the allocator has adjusted the requested size, it searches the free list for a suitable free block (line 18)
  - if there is a fit, then the allocator places the requested block and optianlly splits the excess (line 19), and
    then returns the address of the newly allcoated block

If the allocator cannot fund a fit, it extends the heap with a new free block (lines 24-26), places the requested
block in the new free block, optionally splitting the block (line 27), and the nreturns a pointer to the newly
allocated block

Explicit Free Lists

The implicit free list provides us with a simple way to introduce some basic allocator concepts
  - however, because block allocation time is linear in the total number of heap blocks, the implicit free list is
    not approriate for a general purpsoe allocator (althoug it might be fine for a special purpose allocator where
    the number of heap blocks is known beforehand to be small)

A better approach is to organize the free blocks into some form of explicit data structure
  - since by definition the body of a free block is not neede by the program, the pointers that implement the
    data structure can be stored within the bodies of the free blocks
  - for example, the heap can be organzied as a double linked free list by including a pred (predecessor) and 
    succ (successor) pointer in each free block, as shown in figure 9.48
  - using a doubly linked list instead of an implicit free list reduces the first fit allocation time from linear in
    the total number of blocks to linear in the number of free blocks
  - however, the time to free a block can be either linear or constant, depending on the policy we choose for
    ordering the blocks in the free list

One approach is to maintain the list in last-in first-out (LIFO) order by inserting newly freed blocks at the
beginning of the list
  - with a LIFO ordering and a first fit placement policy, the allocator inspects the most recently used blocks
    first
  - in this case, freeing a block can be performed in constant time
  - if boundary tags are used, then coalescing can aslo be performed in constant time

Another approach is to maintain the list in address order, where the address of each block in the list is less
then the address of the successor
  - in this case, freeing a block requires a linear-time search to locate the appropriate predecessor
  - the trade-off is that address-ordered first fit enjots better memory utilization then LIFO-ordered first
    fit, approaching the utilization of best fit

A disadvatage if explicit lists in general is that free blocks must be large enough to contain all of the necessary
pointers, as well as the header and possibly the footer
  - this results in a larger minimum block size, and increases the potential for internal fragmentation

Segregated Free Lists

As we have seen, an allocator that uses a single linked list of free blocks requires time linear in the number of
free blocks to allocate a block
  - a popular approach for reducing the allocation time, known generlly as segrated storage, is to maintain multiple
    free lists, where each list holds blocks that are roughly the same size
  - the general idea is to partition the set of all possble block sizes into equivalence classes called size classes
  - there are many ways to define the size classes
  - for example, we might partition the block size by powers of two:

      {1}, {2}, {3, 4}, {5 - 8}, ..., {1025 - 2048}, {2049 - 4096}, {4097 - infinity}
  - or we might assign small blocks to their own size classes and partition large blocks by powers of two

      {1}, {2}, {3}, ..., {1023}, {1024}, {1025 - 2048}, {2049 - 4096}, {4097 - infinity}

  - the allocator maintains an array of free lists, with one free list per size class, ordered by increasing size
  - when the allocator needs a block of size n, it searches the appropriate free list
  - if it cannot find a block that fits, it searches the next list, and so on

The dynamic storage allocation literatur describes dozens of variants of segrated storage that differ in how they
define size classes. when they perform coalescing, when they request additional heap memory from the operating
system, whether they allow splitting, and so forth.
  - to give you sense of what is possible, we will describe tow of the basic approaches: simple segregated storage
    and segregated fits

Simple Segregated Storage

With simple segregated storage, the free list for each size class contains same-sized blocks, each the size of the
largest element of the size class
  - for example, if some size class is defined as {17 - 32}, then the free list for that class consists entirely
    of blocks of size 32

To allocate a block of some given size, we check the appropriate free list
  - if the list is not empty, we simply allocate the first block in its entirety
  - free blocks are never split to satisfy allocation requests
  - if the list is empty, the allocator requests a fixed-sized chunk of additional memory from the operating system
    (typically a multiple of the page size), divides the chunk into equla sized blocks, and links the blocks together
    to form the new free list
  - to free a block, the allocator simply inserts the block at the front of the appropriate free list

There a number of advantages to this simple cheme
  - allocating and freeing blocks are both fast constant-time operations
  - further, the combination of the same-sized blocks in each chunk, no splitting, and no coaescing means that there
    is very little per-block memory overhead
  - since each chunk has only the same sized blocks, the size of an allocated block can be inferred from its address
  - since there is no coalescing, allocated block do not need an allocated/free flag in the header
  - thus, allocated blocks require no headers, since there is no coalescing, they do not require footers either
  - since allocate and free operations insert and delete blocks at the beginning of the free list, the list need
    only be singly linked instead of double linked
  - the bottom line is that the only required field in any block is a one-word succ pointer in each free block, and
    thus the minimum block size is only one word

A significant disadvantage is that simple segrated storage is susceptible to internal and external fragmentation
  - internal fragmentation is possible because free blocks are never split
  - worse, certain reference patterns can cause extrernal fragmentation because free blocks are never coalesced

Segregated Fits

With this approach, the allocator maintains an array of free lists
  - each free list is associated with a size class and is organized as some kind of explicit or implicit list
  - each list contains potentially different-sized blocks whose sizes are members of the same size class
  - there are many variants of segregatd fit allocators
  - here we describe a simple version

To allocate a block, we determine the size class of the request and do a first-fit search of the appropriate free
list for a block that fits
  - if we find one, then we optionally split it and insert the fragment in the appropriate free list
  - if we cannot find a block that fits, then we search the free list for the next larger size class
  - we repeat until we find a block that fits
  - if none of the free lists yields a block that fits, then we request additional heap memory from the operating
    system, allocate the block out of this new heap memory, and place the remainder in the approprate size class
  - to free a block, we coalesce and place the result on the appropriate free list

The segregated fits approach is a popular choice with production-quality allocators such as the GNU malloc package
provided in the C standard library b/c it is both fast and memory efficient
  - search times are reduced b/c searches are limited to particular parts of the heap instead of the entire heap
  - memory utilization can improve b/c of the interesting fact that a simple first-fit serach of a segregated free
    list approximates a best-fit search of the entire heap

Buddy Systems

A buddy system is a special case of segregated fits where each size class is a power of two
  - the basic idea is that given a heap of 2^m words, we maintain a separate free list for each block size 2^k,
    where 0 <= k <= m
  - requested block sizes are rounded up to the nearest power of two
  - originally, there is one free block of size 2^m words

To allocate a block size 2^k, we find the first available block of size 2^j, such that k <= j <= m
  - if j = k, then we are done
  - otherwise, we recursively split the block in half until j = k
  - as we perform this splitting, each remaining half (known as a buddy) is placed on the appropriate free list
  - to free a block of size 2^k, we continue coalescing with the free
  - when we encounter an allocated buddy, we stop the coalesing

A key fact about buddy systems is that given the address and size of a block, it is easy to computer the address
of its buddy
  - for example, a block of size 32 bytes with address

      xxx...x00000

    has its buddy at address

      xxx...x10000

  - in other words, the addresses of a block and its buddy differ in exactly one bit position

The major advantage of a buddy system allocator is its fast searching and coalescing
  - the major disadvantage is that the power-of-two requirement on the block size can cause significant internal
    fragmentation
  - for this reason, buddy system allocators are not appropriate for general purpose workloads
  - however, for certain application-specific workloads, where block sizes are known in advance to be powers of
    two, buddy system allocatos have a certain appeal

Garbage Collection

With an explicit allocator such as the C malloc package, an apllication allocates and frees heap blocks by making
calls to malloc and free
  - it is the application's responsibility to free any allocated blocks that it no longer needs
  - failing to free allocated blocks is a common programming error
  - for eample, consider the following C function that allocates a block of temporary storage as part of its
    processing

      void gerbage() {
	int *p = (int *)Mallox(15213);

	return;	  // array p is garbage at this point
      }

  - since p is no longetr needed by the program, it should have been freed before garbage returned
  - unfortunately, the programmer has forgotten to free the block
  - it remains allocated for the lifetime of the program, needlessly occupying heap space that could be used to
    satisfy subsequent allocation requests

A garbage collector is a dynamic storage allocator that automatically frees allocated blocks that are no longer
needed by the program
  - such block are known as garbage (hence the term garbage collector)
  - the process of automatically reclaiming heap storage is known as garbage collection
  - in a system that supports garbage collection, applications explicitly allocate heap blocks but never explicitly
    free them
  - in the context of a C program, the applicaiton calls malloc, but never calls free
  - instead, the garbage collector periodically identifies the garbage blocks and makes the appropriate calls to
    free to place those blocks back on the free list

Garbage collection backs back to Lisp system developed by John McCarthy at MIT in the early 1960s
  - it is an important part of modern language systems such as Java, ML, Perl, and Mathematica, and it remains an
    active and important area of research
  - the literature describes an amazing number of approaches for garbage collection
  - we will limit our discussion to McCarthy's original Mark & Sweep algorithm, which is interesting because it can
    be built on top of an existing malloc package to provide garbage collection for C and C++ programs

Garbage Collector Basics

A garbage collector views memory as a directed reachability graph of the form shown in figure 9.49
  - the nodes of the graph are paritioned into a set of root nodes and a set of heap nodes
  - eac heap node corresponds to an allocated block in the heap
  - a directed edge p -> q means that some location in block p points to some location in block q
  - root nodes correspond to locations not in the heap that contain pointers into the heap
  - these locations can be registers, variables on the stack, or global variables in the read-write data are of
    virtual memory

We say that a node p is reachable if there exists a directed path from any root node to p
  - at any point in time, the unreachable nodes correspond to garbage that can never be used again by the
    application
  - the role of the garbage collector is to maintain some representaion of the reachability graph and periodically
    reclaim the unreachable nodes by freeing them and returning them to the free list

Garbage collectos for languages like ML and Java, which exert tight control onver how applications create and use
pointers, can maintain an exact representation of the reachability graph, and thus can reclaim all garbage
  - however, collectors for languages like C and C++ cannot in general maintain exact representations of the reach-
    ability graph
  - such collectors are known as conservative garbage collectos
  - they are conservative in the sense that each reachable block is correctly identified as reachable, while some
    unreachable nodes might be incorrectly identified as reachable

Collectors can provie their service on demand, or they can run as separate threads in parallel with the application,
continuously updating the reachability graph and reclaiming garbage
  - for example, consider how we might incorporate a conservative collector in C programs into an existing malloc
    package, shown in figure 9.50
  - the application calls malloc in the usual manner whenever it needs heap space
  - if malloc is unable to find a free block that fits, then it calls the garbage collector in hopes of reclaiming
    some garbage to the free list
  - the collector identifies the garbage blocks and returns them to the heap by calling the free function
  - the key idea is that the collector calls free instead of the application
  - when the call to the collector returns, malloc tries again to find a free block that fits
  - if that fails, then it can ask the operating system for additional memory
  - eventually malloc returns a pointer to the requested block (if successful) or the NULL pointer (if unsuccessful)

Mark & Sweep Garbage Collectors

A Mark & Sweep garbage collector consists of a mark phase, which marks all reachable and allocated descendents of
the root nodes, followed by a sweep phase, which frees each unmarked allocated block
  - typically, one of the spare low-order bits in the block header is used to indicate whether the block is marked
    or not

Our description of Mark & Sweep will assume the following functions, where ptr is defined as typefdef void *ptr
  - ptr isPtr(ptr p): is p points to some word in an allocated block, returns a pointer b to the beginning of the
    block. Returns NULL otherwise
  - int blockMarked(ptr b): returns true is block b is already marked
  - int blockAllocated(ptr b): returns true is block b is allocated
  - void markBlocked(ptr b): Marks block b.
  - int length(ptr b): Returns the length in words (excluding the header) of block b
  - void unmarkBlock(ptr b): chages the status of block b from marked to unmarked
  - ptr nextBlock(ptr b): returns the successor of block b in the heap

The mark phase calls the mark function shown in Figure 9.51 (a) once for each root node
  - the mark function returns immediately if p does not point to an allocated and unmarked heap block
  - otherwise, it marks the block and calls itself recursivel and each word in block
  - each call to the mark function marks any unmarked and reachable descendants of some root node
  - at the end of the mark phase, any allocated block that is not marked is guaranteed to be unreachable and,
    hence, garbage that can be reclaimed in the sweep phase

The sweep phase is a single call to the sweep function shown in figure 9.51 (b)
  - the sweep function iterates over each block in the heap, freeing any unmarked alloocated blocks (i.e., garbage)
    that it encounters

Figure 9,52 shows a graphical interpretation of Mark & Sweep for a small heap
  - block boundaries are indicated by heavy lines
  - each square corresponds to a word of memory
  - each block has a one word header, which is either marked or unmarked
  - initially the heap in figure 9.52 consists of six allocated blocks each of which is unmarked
  - block 3 contains a pointer to block 1
  - block 4 contains pointers to blocks 3 and 6
  - after the mark phase, blocks 1, 3, 4, and 6 are marked b/c they are reachable from the root
  - blocks 2 and 5 are unmarked because they are unreachable
  - after the sweep phase, the two unreachable blocks are reclaimed to the free list

Conservative Mark & Sweep for C Programs

Mark & Sweep is an appropriate approach for garbage collecting in C programs because it works in place without
moving any blocks
  - however, the C lanuage poses some interesting challenges for the implementation of the isPtr function

First, C does not tag memory location with any type of information
  - thus there is no obvious way for isPtr to determine if its input parameter p is a pointer or not
  - second, even it we were to know that p was a pointer, there would be no obvious way for isPtr to determine
    whether p points to some location in the payload of an allocated block

On solution to the latter problem is to maintin the set of allocated blocks as a balanced binary tree that maintains
the invariant that all blocks in the left subtree are located at smaller addresses and all blocks in the right are
located in larger addresses
  - as shown in figure 9.53 this requires two additionsl fields (left adn right) in the header of each allocated
    block
  - each field points to the header of some allocated block


The isPtr(ptr p) function uses the tree to perform a binary search of the allocated blocks
  - at each step, it relies on the size field in the block header to determine if p falls within the context
    of the block

The balanced tree approach is correct in the sense that it is guaranteed to mark all of the nodes that are reachable
from the roots
  - this is a necessary guarantee, as application users would certainly not appreciate having their allocated blocks
    prematurely returned to the free list
  - hoowever, it is conservative in the sense that it may incorrectly nark blocks that are actually unreachable,
    and thus it may fail to free some garabage
  - while this does not affect the correctness of application programs, it can result in unnecessary external
    fragmentation

The fundamental reason Mark & Sweep collectors for C programs must be conservative is that the C language does not
tag memory locations with type information
  - thus, scalars like ints or floats can masquerade as pointers
  - for example, suppose that some reachable block contains an int in its payload whose value happens to correspond
    to an address in the payload of some other allocated block b.
  - there is no way for the collector to infer that the data is really an int and not a pointer
  - therefore, the allocator must conservatively mark b as reachable, when in fact it might not be

Common Memory-Related Bugs in C Programs

Managing and using virtual memory can be a difficult and error-prone task for C programmers
  - memory related bugs are among the most frigtening because they often manifest themselves at a distance, in both
    time and space, from the source of the bug
  - write the wrong data to the wrong location, and your program can run for hours before it finally fails in some
    distant part of the program
  - We conclude our discussion of virtual memory with a discussion of some of the common memory-related bugs

Dereferencing Bad Pointers

As we learned in Section 9.7.2, there are large holes in the virtual address space of a process that are not mapped
to any meaningful data
  - if we attempt to dereference a pointer into one of these holes, the operating system will terminate our
    program with a segmentation fault
  - also some areas of virtual memory are read-only
  - attempting to write to one of these areas terminates the program with a protection exception

A common example of dereferencing a bad pointer is the classic scanf bug
  - suppose we want to use scanf to read an integer from stdin into a variable
  - the correct way to do this is to pass scanf a format string and the address of the variable

      scanf("%d", &val);

  - however, it is easy for new C programmers (and experienced ones too!) to pass thhe contents of val instead of 
    its address:

      scand("%d", val);

  - in this case, scanf will interpret the contents of val as an address and attempt to write a word to that location
  - in the best case, the program terminates immediately with an exception
  - in the worst case, the contents of val corresponds to some valid read/write area of virtual memory, and we
    overwirte memory, usually with disastrous and baffling consequences much later

Reading Uninitialized Memory

While bss memory locations (such as uninitialized global C variables) are always initialized to zeros by the loader,
this is not true for heap memory
  - a common error is to assume that heap memory is initialized to zero

      //  Return y = AX
      int *matvex( int **A, int *x, int n ) {
	int i. j;

	int *y = ( int * )Malloc( n * sizeof( int ) );

	for( i = 0; i < n; i++ )
	  for( j = 0; j < n; j++ )
	    y[ i ] += A[ i ][ j ] * x[ j ];
	return y;
      }

  - in this example, the programmer has incorrectly assumber that vector y has been initialized to zero
  - a correct implementation would explicitly zero y[ i ], or use calloc

Allowing Stack Buffer Overflows

As we saw in Section 3.12, a program has a buffer overflow bug if it writes to a target buffer on the stack wihtout
examining the size of the input string
  - for example, the following function has a bufer overfow bug because the gets function copies an arbitrary length
    string to the buffer
  - to fix this, we would need to use the fgets function, which limits the size of the input string

      void bufoverflow() {
	char buf[ 64 ];

	gets( buf );	  //  here is the stack buffer overflow bug
	return;
      }

Assuming that Pointers and the Objects They Point to Are The Same Size

One common mistake is to assume that pointers to objects are the same size as the object they point to

 1   //	Create an nxm array
 2   int **makeArray1( int n, int m ) {
 3     int i;
 4     int **A = ( int ** )Malloc( n * sizeof( int ) );
 5
 6     for( i = 0; i < n; i++ )
 7	A[ i ] = ( int * )Malloc( m * sizeof( int ) );
 8     return A;
 9   }

  - the intent here is to create an array of n pointers, each of which points to an array of m ints
  - however since the programmer has written sizeof( int ) instead of sizeof( int * ) in line 4, the code actually
    creates an array of ints

This code will run fine on machines where ints and pointers to ints are the same size
  - but if we run this code on a machine line the Core i7, where a pointer is larger than an int, then the loop in
    lines 6-7 will write past the end of the A array
  - since one of there words will likely be the boundary tag footer of the allocated block, we may not discover
    the error until we free the block much later in the program, at which point the coalescing code in the allocator
    will fail dramatically and for no apparent reason
  - this is an insidious example of the kind of "action at a distance" that is so typical of memory related programming
    bugs

Making Off-by-One Errors

Off-by-one errors are another source of overwriting bugs:

 1   //	Create an nxm array
 2   int **makeArray2( int n, int m )
 3   {
 4	int i;
 5	int **A = ( int ** )Malloc( n * sizeof( int * ) );
 6
 7	for( i = 0; i <= n; i++ )
 8	  A[ i ] = ( int * )Malloc( m * sizeof( int ) );
 9	return A;
 10  }

  - this is another version of the program in the previous section
  - here we have created an n-element array of pointers in line 5, but then tried to initialize n + 1 of its
    elements in lines 7 and 8, in the process overwritting some memory that folloes the A array

Referencing a Pointer Instead of the Object It Points to

If we are not careful about the precedence and associativity of C operators, then we incorrectly manipulate a pointer
instead of the opbect it points to
  - for example, consider the following function, whose purpose is to remove the first item in a binary heap of
    *size items, and then reheapify the remaining *size - 1 items

      1	  int *binheapDelete( int **binheap, int *size )
      2	  {
      3	    int *packet = binheap[ 0 ];
      4
      5	    binheap[ 0 ] = binheap[ *size - 1 ];
      6	    *size--;  //  this should be (*size)--
      7	    heapify( binheap, *size, 0 );
      8	    return( packet );
      9	  }

  - in line 6, the intent is to decrement the integer value pointed to by the size pointer
  - however, because the unary -- and * operators have the same precedence and associate from right to left, the
    code in line 6 actually decrements the pointer itself instead of the integer value that it points to
  - if we are lucky, the program will crash immediately, but more likely we will be left scratching our heads when
    the program produces an incorrect answer much later in its execution
  - the moral here is to use parentheses whenever in doubt about the precedence and associativity
  - for example, in line 6 we should have clearly stated out intent by using the expression ( * size )--

Misunderstanding Pointer Arithmetic

Another common mistake is to forget that arithmetic operations on pointers are performed in units that are the size
of the objects they point to, which are not neccessarily bytes
  - for example, the intent of the following function is to scan an array of ints and return a pointer to the first
    occurence of val

      1	  int *seach( int *p, int val )
      2	  {
      3	    while( *p && *p != val )
      4	      p += sizeof( int );     //  should be p++;
      5	    return p;
      6	  }

  - because line 4 increments the pointer by 4 (the number of bytes in an integer) each time through the loop, the
    function incorrectly scans every fourth integer in the array

Referencing Nonexistent Variable

Naive C programmers who do not understand the stack discipline will sometimes reference local variables that are no
lnger valid, as in the following example

    int *stackref() {
      int val;
      return &val;
    }

  - this function returns a pointer (say p) to a local variable on the stack and then pops its stack frame
  - although p still points to a valid memory address, it no longer pointe to a valid variable
  - when other functions are called in the program, the memory will be reused for their stack frames
  - later, it the program assigns some value to *p, then it might actually be modifying an entry in another function's
    stack frame, with potentially disastrous and baffling consequences

Referencing Data in Free Heap Blocks

A similar error is to reference data in heap blocks that have already been freed
  - for example, consider the following example, which allocates an integer array x in line 6, prematurely frees
    block x in line 10, and then later references it in line 14

      1	  int *heapref( int n, int m )
      2	  {
      3	    int i;
      4	    int *x, *y;
      5
      6	    x = ( int * )Malloc( n * sizeof( int ) );
      7
      8	    /* ... */	//  Other calls to malloc and free go here
      9
      10    free( x );
      11
      12    y = ( int * )Malloc( m * sizeof( int ) );
      13    for( i = 0; i < m; i++ )
      14      y[ i ] = x[ i ]++;    //	Oops! x[ i ] is a word in a free block
      15
      16    return y;
      17  }

  - depending on the pattern of malloc and free calls that occur lines 6 - 10, when the program references x[ i ] in
    line 14, the array x might be part of some other allocated heap block and have been overwritten
  - as with many memory-related bugs, the error will only become evident later in the program when we notice that the
    values in y are corrupted

Introducing Memory Leaks

Memory leaks are slow, silent killers that occur when programmers inadvertently create garbage in the heap by forgetting
to free allocated blocks
  - for example, the following function allocates a heap block x and returns without freeing it

      1	  void leak( int n )
      2	  {
      3	    int *x = ( int * )Malloc( n * sizeof( int ) );
      4
      5	    return;   //  x is garbage at this point
      6	  }

  - if leak is called frequently, then the heap will gradually fill up with garbage, in the worst case consuming the
    entire virtual address space
  - memory leaks are particularly serious for programs such as daemons and servers, which by definition never terminate

Summary

Virtual memory is an abstraction of main memory
  - processors that support virtual memory reference main memoyr using a form of indirection known as virtual
    addressing
  - the processor generates a virtual address, which is translated into a physical address before being sent to main
    memory
  - the transaltion of addresses from a firtual address space to a physical address space requires close cooperation
    between hardware and software
  - dedicated hardware translates virtual addresses using page tables whose contents are supplied by the operating
    system

Virtual memory provides three important capabilities
  - first, it automatically caches recently used contents of the virtual address spaces stored on disk in main
    memory
  - the block in a virtual memory cache is known as a page
  - a reference to a page on disk triggers a page fault that transfers control to a fault handler in the operating
    system
  - the fault handler copies the page from disk to the main memory cache, writing back the evicted page if necessary
  - second, virtual memory simplifies memory management, which in turn simplifies linking, sharing data between
    processes, the allocation of memory for processes, and program loading
  - finally, virtual memory simplifies memory protection by incorporating protection bits into every page table
    entry

The process of address translation must be integrated with the operation of any hardware caches in the system
  - most page table entries are located in the L1 cache, but the cost of accessing page table entries from L1 is
    usually eliminated by an on-chip cache of page table entired call a TLB

Modren systems initialize chunks of virtual memory by associating them with chunks of files on disk, a process known
as memory mapping
  - memory mapping provides an efficient mechanism for sharing data, creating new processes, and loading programs
  - applicaitons can manually create and delete areas of the virtual address space using the mmap function
  - however, most programs rely on a dynamic memory allocator such as malloc, which manages memory in an area of the
    virtual address space called the heap
  - dynamic memory allocators are applicaiton-level program with a system-level feel, directly manipulating memory
    without much help from the type system
  - allocators come in two flavors
  - explicit allocators require applications to explicitly free their memory blocks
  - implicit allocators (garbage collectors) free unused and unreahcable blocks automatically

Managing and using memory is a difficult and error-prone taks for C programmers
  - examples of common errors include dereferencing bad pointers, reading uninitialized memory, allowing stack
    buffer overflows, assuming that pointers and the object they point to are the same size, referencing a pointer
    instead of the object it points to, misunderstanding pointer arithmetic, referencing nonexistent variables, and
    introducing memory leaks
